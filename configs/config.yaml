device: cuda  # Device to use for training (e.g., "cuda" or "cpu")
classifier_hidden: 512
contrastive:
  lambda_enc: 0.5
  lambda_int: 2.0
  temperature: 0.1
dataset:
  mask_prob: 0.1
  columns:
    label: label
    peptide: seq_2
    tcr: seq_1
  test_csv: test.csv
  train_csv: train.csv
  val_csv: val.csv
esm:
  encoder1: esm2_t33_650M_UR50D
  encoder2: esm2_t33_650M_UR50D

evaluation:
  batch_size: 32
  metrics:
  - auc
  - accuracy
  
  wandb:
    log: true
use_lora: True
lora:
  presets:
    esmc_300m:
      alpha: 32
      bias: none
      dropout: 0.1
      layers_to_transform:
      - 30
      - 29
      - 28
      - 27
      - 26
      r: 8
      task_type: FEATURE_EXTRACTION
    esmc_600M:
      alpha: 32
      bias: none
      dropout: 0.1
      layers_to_transform:
      - 30
      - 29
      - 28
      - 27
      r: 16
      task_type: FEATURE_EXTRACTION
    esm2_t12_35M_UR50D:
      alpha: 32
      bias: none
      dropout: 0.1
      layers_to_transform:
      - 12
      - 11
      - 10
      - 9
      - 8
      r: 16
      task_type: FEATURE_EXTRACTION
    esm2_t33_650M_UR50D:
      alpha: 32
      bias: none
      dropout: 0.1
      layers_to_transform:
      - 32
      - 31
      - 30
      - 29
      - 28
      r: 8
      task_type: FEATURE_EXTRACTION
    esm2_t6_8M_UR50D:
      alpha: 32
      bias: none
      dropout: 0.05
      layers_to_transform:
      - 6
      - 5
      - 4
      - 3
      - 2
      r: 8
      task_type: FEATURE_EXTRACTION
model:
  checkpoint_dir: runs/best_model/

training:
  second_contrastive: True
  batch_size: 32
  dropout: 0.08
  early_stopping:
    patience: 5
  epochs: 200
  focal_gamma: 2.0
  lambda_int: 2.0
  lora_alpha: 32
  lora_dropout: 0.0
  lora_r: 32
  lr: 0.00005
  output_dir: 
  temperature: 0.05
  weight_decay: 0.00005
wandb:
  project: 
  run: 
